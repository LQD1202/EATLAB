from ultralytics import YOLO
import subprocess
import cv2
import os
from deep_sort_realtime.deepsort_tracker import DeepSort
import argparse
def model_index(index):
    if index == 1:
        # === B∆Ø·ªöC 1: C·∫ÆT VIDEO ===
        input_video = "./dataset/1461_CH01_20250607193711_203711.mp4"
        output_video_15min = "./dataset/temp_15min.mp4"
        start_time = "00:10:00"
        duration = "00:05:00"

        print("üî™ C·∫Øt video b·∫±ng ffmpeg...")
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", start_time,
            "-i", input_video,
            "-t", duration,
            "-c", "copy",
            output_video_15min
        ], check=True)
        print("‚úÖ C·∫Øt video th√†nh c√¥ng:", output_video_15min)

        # === B∆Ø·ªöC 2: KH·ªûI T·∫†O MODEL V√Ä TRACKER ===
        model_path = "./yolov12m-cam1/yolov12m-cam13/weights/best.pt"
        model = YOLO(model_path)
        tracker = DeepSort(max_age=300)

        # === B∆Ø·ªöC 3: SETUP VIDEO OUTPUT ===
        cap = cv2.VideoCapture(output_video_15min)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        output_video = "./dataset/temp_15min_detected.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

        # === V√ôNG QUAN T√ÇM ===
        x_center = (width // 2) + 100
        y_center = (height // 2) + 200
        y_thresh = 200

        # === BI·∫æN THEO D√ïI ===
        frame_count = 0
        count_pizza = 0
        tracked_ids = set()

        # === THAM S·ªê MODEL ===
        class_id_pizza = 67  # ID class "pizza"
        conf_thresh = 0.9

        # === V√íNG L·∫∂P D·ª∞ ƒêO√ÅN ===
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = model(
                frame,
                conf=conf_thresh,
                classes=[class_id_pizza],
                agnostic_nms=False,
                max_det=100,
                device="cuda:1",
                verbose=False
            )

            detections = []
            boxes = results[0].boxes
            annotated_frame = frame.copy()

            if boxes is not None and boxes.xyxy is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].item())
                    cls = int(box.cls[0].item())
                    w, h = x2 - x1, y2 - y1
                    detections.append(([x1, y1, w, h], conf, str(cls)))

            # === C·∫¨P NH·∫¨T TRACKING ===
            tracks = tracker.update_tracks(detections, frame=frame)

            for track in tracks:
                if not track.is_confirmed() or track.track_id is None:
                    continue

                track_id = track.track_id
                x1, y1, x2, y2 = map(int, track.to_ltrb())

                # T√¨m ƒë·ªô tin c·∫≠y g·∫ßn ƒë√∫ng
                conf = 0.0
                for det in detections:
                    bbox, det_conf, det_cls = det
                    x, y, w, h = bbox
                    if abs(x - x1) < 10 and abs(y - y1) < 10:
                        conf = det_conf
                        break

                # T√≠nh t√¢m object
                x_center_obj = (x1 + x2) // 2
                y_center_obj = (y1 + y2) // 2

                # L·ªçc object b√™n ph·∫£i khu v·ª±c quan t√¢m
                if x_center_obj > x_center and y_thresh < y_center_obj < y_center and conf >= conf_thresh:
                    if track_id not in tracked_ids:
                        tracked_ids.add(track_id)
                        count_pizza += 1

                    # V·∫Ω box + ID
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 3)
                    cv2.putText(annotated_frame, f"ID {track_id} ({conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng pizza ƒë√£ ƒë·∫øm
            cv2.putText(annotated_frame, f"Pizza Count: {count_pizza}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)

            out.write(annotated_frame)
            frame_count += 1
            print(f"üß† ƒê√£ x·ª≠ l√Ω frame {frame_count}")

        cap.release()
        out.release()
        cv2.destroyAllWindows()
        print(f"üé¨ Ho√†n t·∫•t! Video l∆∞u t·∫°i: {output_video}")
    elif index == 2:
        # === B∆Ø·ªöC 1: C·∫ÆT VIDEO ===
        input_video = "./dataset/1465_CH02_20250607170555_172408.mp4"
        output_video_15min = "./dataset/temp_15min.mp4"
        start_time = "00:04:50"
        duration = "00:01:00"

        print("üî™ C·∫Øt video b·∫±ng ffmpeg...")
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", start_time,
            "-i", input_video,
            "-t", duration,
            "-c", "copy",
            output_video_15min
        ], check=True)
        print("‚úÖ C·∫Øt video th√†nh c√¥ng:", output_video_15min)

        # === B∆Ø·ªöC 2: KH·ªûI T·∫†O MODEL V√Ä TRACKER ===
        model_path = "./yolov12m-cam2/yolov12m-cam26/weights/best.pt"
        model = YOLO(model_path)
        tracker = DeepSort(max_age=300)

        # === B∆Ø·ªöC 3: ƒê·ªåC VIDEO V√Ä TRACK ===
        cap = cv2.VideoCapture(output_video_15min)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        output_video = "./dataset/temp_15min_detected_cam2.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

        # === TH∆Ø M·ª§C L∆ØU FRAME ===
        output_frame_dir = "./dataset/test/frames"
        os.makedirs(output_frame_dir, exist_ok=True)

        frame_count = 0
        tracked_ids = set()

        # === ƒê∆Ø·ªúNG CH√âO: y = -x + b ===
        b = height + 100  # ƒëi·ªÅu ch·ªânh n·∫øu mu·ªën ƒë∆∞·ªùng ch√©o cao h∆°n ho·∫∑c th·∫•p h∆°n

        # === THAM S·ªê PH√ÅT HI·ªÜN ===
        conf_thresh = 0.9
        target_cls = 0  # l·ªõp c·∫ßn ph√°t hi·ªán (v√≠ d·ª•: 0 = pizza)

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = model(
                frame,
                conf=conf_thresh,
                classes=[target_cls],
                agnostic_nms=False,
                max_det=100,
                device="cuda:1",
                verbose=False
            )

            detections = []
            boxes = results[0].boxes

            if boxes is not None and boxes.xyxy is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].item())
                    cls = int(box.cls[0].item())
                    w, h = x2 - x1, y2 - y1
                    detections.append(([x1, y1, w, h], conf, str(cls)))

            tracks = tracker.update_tracks(detections, frame=frame)
            annotated_frame = frame.copy()

            pt1 = (0, b)
            pt2 = (width, -width + b)
            cv2.line(annotated_frame, pt1, pt2, (255, 0, 255), 2)

            for track in tracks:
                if not track.is_confirmed() or track.track_id is None:
                    continue

                track_id = track.track_id
                x1, y1, x2, y2 = map(int, track.to_ltrb())

                # T√¨m l·∫°i ƒë·ªô tin c·∫≠y g·∫ßn ƒë√∫ng t·ª´ detection
                conf = 0.0
                for det in detections:
                    bbox, det_conf, det_cls = det
                    x, y, w, h = bbox
                    if abs(x - x1) < 10 and abs(y - y1) < 10:
                        conf = det_conf
                        break

                # T√¢m object
                x_center_obj = (x1 + x2) // 2
                y_center_obj = (y1 + y2) // 2

                # Ki·ªÉm tra n·∫øu object n·∫±m b√™n ph·∫£i (d∆∞·ªõi) ƒë∆∞·ªùng ch√©o
                if conf >= conf_thresh:
                    tracked_ids.add(track_id)

                    # V·∫Ω bbox + ID
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
                    cv2.putText(annotated_frame, f"ID {track_id} ({conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)

            # V·∫Ω t·ªïng s·ªë object ƒë√£ ƒë·∫øm
            cv2.putText(annotated_frame, f"Pizza Count: {len(tracked_ids)}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)

            # Ghi video v√† l∆∞u frame
            out.write(annotated_frame)
            frame_filename = os.path.join(output_frame_dir, f"frame_{frame_count:05d}.jpg")
            cv2.imwrite(frame_filename, annotated_frame)

            frame_count += 1

        # === K·∫æT TH√öC ===
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        print(f"üé¨ Done! Output video saved to: {output_video}")
        print(f"üñºÔ∏è C√°c frame ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_frame_dir}")
        print(f"üì¶ T·ªïng s·ªë object duy nh·∫•t n·∫±m b√™n ph·∫£i ƒë∆∞·ªùng ch√©o: {len(tracked_ids)}")
    elif index == 3:
        # === B∆Ø·ªöC 1: C·∫ÆT VIDEO ===
        input_video = "./dataset/1462_CH04_20250607210159_211703.mp4"
        output_video_15min = "./dataset/temp_15min.mp4"
        start_time = "00:10:00"
        duration = "00:01:00"

        print("üî™ C·∫Øt video b·∫±ng ffmpeg...")
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", start_time,
            "-i", input_video,
            "-t", duration,
            "-c", "copy",
            output_video_15min
        ], check=True)
        print("‚úÖ C·∫Øt video th√†nh c√¥ng:", output_video_15min)

        # === B∆Ø·ªöC 2: KH·ªûI T·∫†O MODEL V√Ä TRACKER ===
        model_path = "./yolov12m-cam3/yolov12m-cam3/weights/best.pt"
        model = YOLO(model_path)
        tracker = DeepSort(max_age=30)

        # === TH∆Ø M·ª§C L∆ØU FRAME ===
        output_frame_dir = "./dataset/frames"
        os.makedirs(output_frame_dir, exist_ok=True)

        # === VIDEO GHI OUTPUT ===
        cap = cv2.VideoCapture(output_video_15min)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        output_video = "./dataset/temp_15min_detected_cam3.mp4"
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

        frame_count = 0
        tracked_ids = set()

        CONF_THRESHOLD = 0.90  # Ch·ªâ v·∫Ω n·∫øu conf > 0.90

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = model(
                frame,
                conf=CONF_THRESHOLD,
                classes=[0],
                agnostic_nms=False,
                max_det=100,
                device="cuda:1",
                verbose=False,
                iou=0.1
            )

            detections = []
            conf_map = {}  # l∆∞u box: conf ƒë·ªÉ v·∫Ω v·ªÅ sau

            boxes = results[0].boxes
            if boxes is not None and boxes.xyxy is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].item())
                    cls = int(box.cls[0].item())
                    w, h = x2 - x1, y2 - y1
                    detections.append(([x1, y1, w, h], conf, str(cls)))
                    conf_map[(int(x1), int(y1), int(x2), int(y2))] = conf

            # === TRACKING ===
            tracks = tracker.update_tracks(detections, frame=frame)
            annotated_frame = frame.copy()

            for track in tracks:
                if not track.is_confirmed():
                    continue

                track_id = track.track_id
                x1, y1, x2, y2 = map(int, track.to_ltrb())
                track_box = (x1, y1, x2, y2)

                # T√¨m l·∫°i conf g·∫ßn ƒë√∫ng theo IOU ho·∫∑c v·ªã tr√≠ box
                matched_conf = 0.0
                for (bx1, by1, bx2, by2), c in conf_map.items():
                    if abs(bx1 - x1) < 10 and abs(by1 - y1) < 10:
                        matched_conf = c
                        break

                if matched_conf >= CONF_THRESHOLD:
                    tracked_ids.add(track_id)
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
                    cv2.putText(annotated_frame, f"ID {track_id} ({matched_conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)

            # Hi·ªÉn th·ªã ƒë·∫øm s·ªë l∆∞·ª£ng object duy nh·∫•t
            cv2.putText(annotated_frame, f"Pizza Count: {len(tracked_ids)}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)

            # Ghi video v√† l∆∞u frame
            out.write(annotated_frame)
            frame_path = os.path.join(output_frame_dir, f"frame_{frame_count:05d}.jpg")
            cv2.imwrite(frame_path, annotated_frame)

            frame_count += 1

        # === K·∫æT TH√öC ===
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        print(f"üé¨ Done! Output video saved to: {output_video}")
        print(f"üñºÔ∏è C√°c frame ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_frame_dir}")
    elif index == 4:

        # === B∆Ø·ªöC 1: C·∫ÆT VIDEO ===
        input_video = "./dataset/1464_CH02_20250607180000_190000.mp4"
        output_video_15min = "./temp_15min.mp4"
        start_time = "00:22:30"
        duration = "00:01:00"

        print("üî™ C·∫Øt video b·∫±ng ffmpeg...")
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", start_time,
            "-i", input_video,
            "-t", duration,
            "-c", "copy",
            output_video_15min
        ], check=True)
        print("‚úÖ C·∫Øt video th√†nh c√¥ng:", output_video_15min)

        # === B∆Ø·ªöC 2: KH·ªûI T·∫†O MODEL V√Ä TRACKER ===
        model_path = "./yolov12m-cam4/yolov12m-cam46/weights/best.pt"
        model = YOLO(model_path)
        tracker = DeepSort(max_age=30)

        # === TH∆Ø M·ª§C L∆ØU FRAME ===
        output_frame_dir = "./frames"
        os.makedirs(output_frame_dir, exist_ok=True)

        # === VIDEO GHI OUTPUT ===
        cap = cv2.VideoCapture(output_video_15min)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        output_video = "./dataset/temp_15min_detected_cam4.mp4"
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

        frame_count = 0
        tracked_ids = set()

        CONF_THRESHOLD = 0.90  # Ch·ªâ v·∫Ω n·∫øu conf > 0.90

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = model(
                frame,
                conf=CONF_THRESHOLD,
                classes=[0],
                agnostic_nms=False,
                max_det=100,
                device="cuda:1",
                verbose=False,
                iou=0.1
            )

            detections = []
            conf_map = {}  # l∆∞u box: conf ƒë·ªÉ v·∫Ω v·ªÅ sau

            boxes = results[0].boxes
            if boxes is not None and boxes.xyxy is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].item())
                    cls = int(box.cls[0].item())
                    w, h = x2 - x1, y2 - y1
                    detections.append(([x1, y1, w, h], conf, str(cls)))
                    conf_map[(int(x1), int(y1), int(x2), int(y2))] = conf

            # === TRACKING ===
            tracks = tracker.update_tracks(detections, frame=frame)
            annotated_frame = frame.copy()

            for track in tracks:
                if not track.is_confirmed():
                    continue

                track_id = track.track_id
                x1, y1, x2, y2 = map(int, track.to_ltrb())
                track_box = (x1, y1, x2, y2)

                # T√¨m l·∫°i conf g·∫ßn ƒë√∫ng theo IOU ho·∫∑c v·ªã tr√≠ box
                matched_conf = 0.0
                for (bx1, by1, bx2, by2), c in conf_map.items():
                    if abs(bx1 - x1) < 10 and abs(by1 - y1) < 10:
                        matched_conf = c
                        break

                if matched_conf >= CONF_THRESHOLD:
                    tracked_ids.add(track_id)
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
                    cv2.putText(annotated_frame, f"ID {track_id} ({matched_conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)

            # Hi·ªÉn th·ªã ƒë·∫øm s·ªë l∆∞·ª£ng object duy nh·∫•t
            cv2.putText(annotated_frame, f"Pizza Count: {len(tracked_ids)}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)

            # Ghi video v√† l∆∞u frame
            out.write(annotated_frame)
            frame_path = os.path.join(output_frame_dir, f"frame_{frame_count:05d}.jpg")
            cv2.imwrite(frame_path, annotated_frame)

            frame_count += 1

        # === K·∫æT TH√öC ===
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        print(f"üé¨ Done! Output video saved to: {output_video}")
        print(f"üñºÔ∏è C√°c frame ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_frame_dir}")
    elif index == 5:

        # === B∆Ø·ªöC 1: C·∫ÆT VIDEO ===
        input_video = "./dataset/1465_CH02_20250607170555_172408.mp4"
        output_video_15min = "./dataset/temp_15min.mp4"
        start_time = "00:04:50"
        duration = "00:01:00"

        print("üî™ C·∫Øt video b·∫±ng ffmpeg...")
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", start_time,
            "-i", input_video,
            "-t", duration,
            "-c", "copy",
            output_video_15min
        ], check=True)
        print("‚úÖ C·∫Øt video th√†nh c√¥ng:", output_video_15min)

        # === B∆Ø·ªöC 2: KH·ªûI T·∫†O MODEL V√Ä TRACKER ===
        model_path = "./yolov12m-cam5/yolov12m-cam56/weights/best.pt"
        model = YOLO(model_path)
        tracker = DeepSort(max_age=300)

        # === B∆Ø·ªöC 3: ƒê·ªåC VIDEO V√Ä TRACK ===
        cap = cv2.VideoCapture(output_video_15min)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        output_video = "./dataset/temp_15min_detected_cam5.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

        # === TH∆Ø M·ª§C L∆ØU FRAME ===
        output_frame_dir = "./dataset/test/frames"
        os.makedirs(output_frame_dir, exist_ok=True)

        frame_count = 0
        tracked_ids = set()

        # === THAM S·ªê GI·ªöI H·∫†N KHU V·ª∞C ===
        y_center = height // 2                   # ch·ªâ l·∫•y object ph√≠a d∆∞·ªõi
        x_line = width // 2 + 200                # ch·ªâ l·∫•y object b√™n ph·∫£i ƒë∆∞·ªùng th·∫≥ng n√†y

        # === THAM S·ªê PH√ÅT HI·ªÜN ===
        conf_thresh = 0.9
        target_cls = 0  # l·ªõp c·∫ßn ph√°t hi·ªán

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = model(
                frame,
                conf=conf_thresh,
                classes=[target_cls],
                agnostic_nms=False,
                max_det=100,
                device="cuda:1",
                verbose=False
            )

            detections = []
            boxes = results[0].boxes

            if boxes is not None and boxes.xyxy is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].item())
                    cls = int(box.cls[0].item())
                    w, h = x2 - x1, y2 - y1
                    detections.append(([x1, y1, w, h], conf, str(cls)))

            tracks = tracker.update_tracks(detections, frame=frame)
            annotated_frame = frame.copy()

            # === V·∫Ω v√πng l·ªçc: ƒë∆∞·ªùng ngang + ƒë∆∞·ªùng d·ªçc
            cv2.line(annotated_frame, (0, y_center), (width, y_center), (255, 0, 0), 2)  # ngang (xanh d∆∞∆°ng)
            cv2.line(annotated_frame, (x_line, 0), (x_line, height), (255, 255, 0), 2)   # d·ªçc (cyan)

            for track in tracks:
                if not track.is_confirmed() or track.track_id is None:
                    continue

                track_id = track.track_id
                x1, y1, x2, y2 = map(int, track.to_ltrb())

                # T√¨m l·∫°i ƒë·ªô tin c·∫≠y g·∫ßn ƒë√∫ng t·ª´ detection
                conf = 0.0
                for det in detections:
                    bbox, det_conf, det_cls = det
                    x, y, w, h = bbox
                    if abs(x - x1) < 10 and abs(y - y1) < 10:
                        conf = det_conf
                        break

                # T√¢m ƒë·ªëi t∆∞·ª£ng
                x_center_obj = (x1 + x2) // 2
                y_center_obj = (y1 + y2) // 2

                # Ch·ªâ ƒë·∫øm n·∫øu object n·∫±m b√™n ph·∫£i x_line v√† b√™n d∆∞·ªõi y_center
                if conf >= conf_thresh and y_center_obj > y_center and x_center_obj > x_line:
                    tracked_ids.add(track_id)

                    # V·∫Ω bbox v√† ID
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
                    cv2.putText(annotated_frame, f"ID {track_id} ({conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)

            # Ghi t·ªïng s·ªë ID duy nh·∫•t
            cv2.putText(annotated_frame, f"Pizza Count: {len(tracked_ids)}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)

            # Ghi video v√† l∆∞u frame
            out.write(annotated_frame)
            frame_filename = os.path.join(output_frame_dir, f"frame_{frame_count:05d}.jpg")
            cv2.imwrite(frame_filename, annotated_frame)

            frame_count += 1

        # === K·∫æT TH√öC ===
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        print(f"üé¨ Done! Output video saved to: {output_video}")
        print(f"üñºÔ∏è C√°c frame ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_frame_dir}")
        print(f"üì¶ T·ªïng s·ªë object duy nh·∫•t ƒë∆∞·ª£c track (b√™n ph·∫£i ƒë∆∞·ªùng chia): {len(tracked_ids)}")
    else:
        def is_point_below_line(x, y, pt1, pt2):
            x1, y1 = pt1
            x2, y2 = pt2

            if x1 == x2:
                # ƒê∆∞·ªùng th·∫≥ng ƒë·ª©ng => kh√¥ng x√°c ƒë·ªãnh "d∆∞·ªõi"
                return x < x1  # ho·∫∑c x > x1 t√πy logic b·∫°n c·∫ßn
            else:
                # T√≠nh y tr√™n ƒë∆∞·ªùng t·∫°i ho√†nh ƒë·ªô x
                y_on_line = y1 + (y2 - y1) * (x - x1) / (x2 - x1)
                return y > y_on_line  # trong ·∫£nh: tr·ª•c y h∆∞·ªõng xu·ªëng => "d∆∞·ªõi" l√† y l·ªõn h∆°n

        # === B∆Ø·ªöC 1: C·∫ÆT VIDEO ===
        input_video = "./dataset/1467_CH04_20250607180000_190000.mp4"
        output_video_15min = "./dataset/temp_15min.mp4"
        start_time = "00:01:10"
        duration = "00:01:00"

        print("üî™ C·∫Øt video b·∫±ng ffmpeg...")
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", start_time,
            "-i", input_video,
            "-t", duration,
            "-c", "copy",
            output_video_15min
        ], check=True)
        print("‚úÖ C·∫Øt video th√†nh c√¥ng:", output_video_15min)

        # === B∆Ø·ªöC 2: KH·ªûI T·∫†O MODEL V√Ä TRACKER ===
        model_path = "./yolov12m-cam6/yolov12m-cam64/weights/best.pt"
        model = YOLO(model_path)
        tracker = DeepSort(max_age=300)

        # === B∆Ø·ªöC 3: ƒê·ªåC VIDEO V√Ä TRACK ===
        cap = cv2.VideoCapture(output_video_15min)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        output_video = "./dataset/temp_15min_detected_cam6.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))

        # === TH∆Ø M·ª§C L∆ØU FRAME ===
        output_frame_dir = "./dataset/test/frames"
        os.makedirs(output_frame_dir, exist_ok=True)

        frame_count = 0
        tracked_ids = set()

        # === ƒê∆Ø·ªúNG CH√âO: y = -x + b ===
        b = height + 300  # ƒëi·ªÅu ch·ªânh n·∫øu mu·ªën ƒë∆∞·ªùng ch√©o cao h∆°n ho·∫∑c th·∫•p h∆°n

        # === THAM S·ªê PH√ÅT HI·ªÜN ===
        conf_thresh = 0.9
        target_cls = 0  # l·ªõp c·∫ßn ph√°t hi·ªán (v√≠ d·ª•: 0 = pizza)

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = model(
                frame,
                conf=conf_thresh,
                classes=[target_cls],
                agnostic_nms=False,
                max_det=100,
                device="cuda:1",
                verbose=False
            )

            detections = []
            boxes = results[0].boxes

            if boxes is not None and boxes.xyxy is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0].item())
                    cls = int(box.cls[0].item())
                    w, h = x2 - x1, y2 - y1
                    detections.append(([x1, y1, w, h], conf, str(cls)))

            tracks = tracker.update_tracks(detections, frame=frame)
            annotated_frame = frame.copy()

            pt1 = (0, b)
            pt2 = (width, max(0, -width + b))  # ƒë·∫£m b·∫£o to·∫° ƒë·ªô kh√¥ng √¢m

            # V·∫Ω ƒë∆∞·ªùng
            cv2.line(annotated_frame, pt1, pt2, (255, 0, 255), 2)

            for track in tracks:
                if not track.is_confirmed() or track.track_id is None:
                    continue

                track_id = track.track_id
                x1, y1, x2, y2 = map(int, track.to_ltrb())

                # T√¨m l·∫°i ƒë·ªô tin c·∫≠y g·∫ßn ƒë√∫ng t·ª´ detection
                conf = 0.0
                for det in detections:
                    bbox, det_conf, det_cls = det
                    x, y, w, h = bbox
                    if abs(x - x1) < 10 and abs(y - y1) < 10:
                        conf = det_conf
                        break

                # T√¢m object
                x_center_obj = (x1 + x2) // 2
                y_center_obj = (y1 + y2) // 2

                # Ki·ªÉm tra n·∫øu object n·∫±m b√™n ph·∫£i (d∆∞·ªõi) ƒë∆∞·ªùng ch√©o
                if conf >= conf_thresh and is_point_below_line(x, y, pt1,pt2):
                    tracked_ids.add(track_id)

                    # V·∫Ω bbox + ID
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
                    cv2.putText(annotated_frame, f"ID {track_id} ({conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 3)

            # V·∫Ω t·ªïng s·ªë object ƒë√£ ƒë·∫øm
            cv2.putText(annotated_frame, f"Pizza Count: {len(tracked_ids)}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)

            # Ghi video v√† l∆∞u frame
            out.write(annotated_frame)
            frame_filename = os.path.join(output_frame_dir, f"frame_{frame_count:05d}.jpg")
            cv2.imwrite(frame_filename, annotated_frame)

            frame_count += 1

        # === K·∫æT TH√öC ===
        cap.release()
        out.release()
        cv2.destroyAllWindows()
        print(f"üé¨ Done! Output video saved to: {output_video}")
        print(f"üñºÔ∏è C√°c frame ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_frame_dir}")
        print(f"üì¶ T·ªïng s·ªë object duy nh·∫•t n·∫±m b√™n ph·∫£i ƒë∆∞·ªùng ch√©o: {len(tracked_ids)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Ch·ªçn video ƒë·ªÉ inference")
    parser.add_argument(
        "--video", 
        type=int, 
        choices=range(1, 7), 
        required=True,
        help="Ch·ªçn video t·ª´ 1 ƒë·∫øn 6",
        default=1
    )
    args = parser.parse_args()

    video_path = model_index(args.video)

    if video_path is None:
        print("Video kh√¥ng t·ªìn t·∫°i!")
    else:
        print(f"ƒêang ch·∫°y inference tr√™n: {video_path}")